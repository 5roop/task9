{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131072"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydatadir = \"/data1/peterr/bilingual_data_for_ABClf\"\n",
    "outdir = \"/data1/peterr/bilingual_data_for_ABClf_output/tsv\"\n",
    "import os\n",
    "import pandas as pd\n",
    "from ABClf import load_lexicon, get_variant, count_variants, counts_to_category\n",
    "cols = \"URL_en    URL_is    sent_en    sent_is    bleualign_score    paragraph_id_en    paragraph_id_is    deferred_hash_en    deferred_hash_is          bifixer_hash    bifixer_score    bicleaner_ai_score\".split()\n",
    "lex = load_lexicon()\n",
    "\n",
    "\n",
    "import parse\n",
    "def get_domain(url:str) -> str:\n",
    "    pattern = \"{protocol}://{domain}/{rest}\"\n",
    "    p = parse.compile(pattern)\n",
    "    modified_pattern = \"{protocol}://{domain}/\"\n",
    "    mp = parse.compile(modified_pattern)\n",
    "    try:\n",
    "        parse_result = p.parse(url)\n",
    "        domain = parse_result[\"domain\"]\n",
    "    except TypeError:\n",
    "        parse_result = mp.parse(url)\n",
    "        domain = parse_result[\"domain\"]\n",
    "    return domain\n",
    "\n",
    "\n",
    "\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "csv.field_size_limit(131072<<4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mk\n",
      "en-mk.deduped.txt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for file in ['en-mk.deduped.txt']:\n",
    "#for file in os.listdir(mydatadir):\n",
    "    if not file.endswith(\".txt\"):\n",
    "        continue\n",
    "    lang = file[3:5]\n",
    "    print(lang)\n",
    "    \n",
    "    endpath = os.path.join(\n",
    "            outdir,\n",
    "            file+\".ABClf_output.txt\",\n",
    "        )\n",
    "    print(file)\n",
    "    df = pd.read_csv(\n",
    "        os.path.join(\n",
    "            mydatadir,\n",
    "            file),\n",
    "        sep=\"\\t\",\n",
    "        engine=\"python\",\n",
    "        quoting=3,\n",
    "        on_bad_lines=\"warn\")\n",
    "    \n",
    "    df.columns = [col.replace(\"_is\", f\"_{lang}\") for col in cols]\n",
    "\n",
    "\n",
    "    s = df.groupby(\"URL_en\")[\"sent_en\"].apply(\" \".join)\n",
    "    ndf = pd.DataFrame(s)\n",
    "    ndf[\"variant_unbalanced\"] = ndf.sent_en.apply(lambda s: get_variant(s, lex=lex))\n",
    "    ndf[\"per_token_unbalanced\"] = ndf.sent_en.apply(lambda s: count_variants(s, lex)[1])\n",
    "    ndf[\"domain\"] = [get_domain(url) for url in ndf.index.values]\n",
    "    ndf[\"URL_en\"] = ndf.index\n",
    "\n",
    "    df = pd.merge(\n",
    "            df,\n",
    "            ndf[['variant_unbalanced', 'per_token_unbalanced', 'domain',]],\n",
    "            how=\"left\",\n",
    "            left_on=\"URL_en\",\n",
    "            right_index=True,\n",
    "            sort=False,\n",
    "            suffixes=(\"_x\", \"_y\"),\n",
    "            copy=True,\n",
    "            indicator=False,\n",
    "            validate=None,\n",
    "            )\n",
    "    df[\"en_domain_level_variant_from_docs\"] = \"UNK\"\n",
    "    df[\"en_domain_level_variant_from_counts\"] = \"UNK\"\n",
    "    counts = df.domain.value_counts()\n",
    "    suitable_domains = counts[counts >= 3].index.values\n",
    "    l = len(suitable_domains)\n",
    "\n",
    "    for i, domain in enumerate(suitable_domains):\n",
    "        # print(f\"Working on domain {i} of {l}\", end=\"\\r\")\n",
    "        # #Calculate a subset of all docs that fit in this domain:\n",
    "        # subset = df.loc[df.domain == domain, [\"variant_unbalanced\", \"per_token_unbalanced\"]]\n",
    "\n",
    "        # subset_counts = subset.variant_unbalanced.value_counts()\n",
    "        # most_frequent = subset_counts.index[0]\n",
    "        # if most_frequent == \"UNK\":\n",
    "        #     try:\n",
    "        #         most_frequent = subset_counts.index[1]\n",
    "        #     except:\n",
    "        #         pass\n",
    "    \n",
    "        # df.loc[df.domain == domain, \"en_domain_level_variant_from_docs\"] = most_frequent\n",
    "        d = dict()\n",
    "        for line in subset.per_token_unbalanced.values:\n",
    "            if not line:\n",
    "                continue\n",
    "            for word, payload in line.items():\n",
    "                variant, count = payload.get(\"variant\"), payload.get(\"count\")\n",
    "                d[variant] = d.get(variant, 0) + 1\n",
    "        variant = counts_to_category(d)\n",
    "        df.loc[df.domain == domain, \"en_domain_level_variant_from_counts\"] = variant\n",
    "\n",
    "    # df.to_csv(\n",
    "    #     endpath,\n",
    "    #     index=False,\n",
    "    #     sep=\"\\t\"\n",
    "    # )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temporal perfomance:\n",
    "* Original: 36 min\n",
    "* Sped up: 32 min\n",
    "* Only doc level: 20 min\n",
    "* Only word level: 13 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['URL_en', 'URL_mk', 'sent_en', 'sent_mk', 'bleualign_score',\n",
       "       'paragraph_id_en', 'paragraph_id_mk', 'deferred_hash_en',\n",
       "       'deferred_hash_mk', 'bifixer_hash', 'bifixer_score',\n",
       "       'bicleaner_ai_score', 'variant_unbalanced', 'per_token_unbalanced',\n",
       "       'domain', 'en_domain_level_variant_from_docs',\n",
       "       'en_domain_level_variant_from_counts'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7f6f5766036ee03d059e365a942add07f79c17033585e9357ee8157d52fe6bb9"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
